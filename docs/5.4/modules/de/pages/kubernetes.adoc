= Kubernetes
:revdate: 2025-06-05
:page-revdate: {revdate}
:page-opendocs-origin: /02.deploying/02.kubernetes/02.kubernetes.md
:page-opendocs-slug: /deploying/kubernetes

== Bereitstellen mit Kubernetes

Sie können Kubernetes verwenden, um separate Manager-, Controller- und Enforcer-Container bereitzustellen und sicherzustellen, dass alle neuen Knoten einen Enforcer bereitgestellt haben. {product-name} benötigt und unterstützt Kubernetes-Netzwerk-Plugins wie Flanell, Weave oder Calico.

Die Beispieldatei wird einen Manager und 3 Controller bereitstellen. Es wird einen Enforcer auf jedem Knoten als Daemonset bereitstellen. Standardmäßig wird das folgende Beispiel auch für den Master-Knoten bereitgestellt.

Im unteren Abschnitt finden Sie die Angabe von dedizierten Manager- oder Controllerknoten mithilfe von Knotenbeschriftungen. 

[NOTE]
====
Es wird nicht empfohlen, mehr als einen Manager hinter einem Load Balancer bereitzustellen (zu skalieren), da mögliche Probleme mit dem Sitzungszustand auftreten. Wenn Sie beabsichtigen, einen PersistentVolume-Anspruch zum Speichern der Sicherung von {product-name}-Konfigurationsdateien zu verwenden, lesen Sie bitte den allgemeinen Abschnitt Backup/Persistente Daten in der Übersicht xref:production.adoc#_backups_and_persistent_data[Bereitstellung von {product-name}].
====

Wenn Ihre Bereitstellung einen integrierten Load Balancer unterstützt, ändern Sie den Typ NodePort in LoadBalancer für die Konsole in der yaml-Datei unten.

{product-name} unterstützt Helm-basierte Bereitstellung mit einem Helm-Chart bei https://github.com/neuvector/neuvector-helm.

Es gibt einen separaten Abschnitt für OpenShift-Anweisungen, und Docker EE auf Kubernetes hat einige spezielle Schritte, die im Docker-Abschnitt beschrieben werden.

=== Bilder auf Docker Hub {product-name}

Die Bilder befinden sich in der {product-name} Docker Hub Registry. Verwenden Sie das entsprechende Versions-Tag für den Manager, Controller, Enforcer und belassen Sie die Version als 'aktuell' für Scanner und Updater. Beispiel:

* neuvector/manager:5.4.3
* neuvector/controller:5.4.3
* neuvector/enforcer:5.4.3
* neuvector/scanner:latest
* neuvector/updater:latest

Bitte aktualisieren Sie die Bildreferenzen in entsprechenden Yaml-Dateien.

Bei der Bereitstellung mit dem aktuellen {product-name} Helm-Diagramm (v1.8.9+) sollten die folgenden Änderungen an values.yml vorgenommen werden:

* Aktualisieren Sie die Registrierung auf docker.io
* Aktualisieren Sie Bildnamen/Tags auf die aktuelle Version am Docker-Hub, wie oben gezeigt
* Lassen Sie das Bild leerPullSecrets

[NOTE]
====
Bei der Bereitstellung über das Rancher Manager 2.6.5+ {product-name} werden Bilder automatisch aus dem gespiegelten Image-Repo der Rancher Registry gezogen und in den Namespace des Vieh-Neuvektor-Systems bereitgestellt.
====


== Einsatz {product-name}

. Erstellen Sie den {product-name} und die erforderlichen Dienstkonten: 
+
--
[,shell]
----
kubectl create namespace neuvector
kubectl create sa controller -n neuvector
kubectl create sa enforcer -n neuvector
kubectl create sa basic -n neuvector
kubectl create sa updater -n neuvector
kubectl create sa scanner -n neuvector
kubectl create sa registry-adapter -n neuvector
kubectl create sa cert-upgrader -n neuvector
----
--
. (*Optional*) Erstellen Sie die {product-name} Pod Security Admission (PSA) oder Pod Security Policy (PSP). Wenn Sie die Pod-Sicherheitszulassung (alias Pod-Sicherheitsstandards) in Kubernetes 1.25+ oder Pod-Sicherheitsrichtlinien (vor 1.25) in Ihrem Kubernetes-Cluster aktiviert haben, fügen Sie Folgendes für {product-name} hinzu (z. B. nv_psp.yaml). 
+
[NOTE]
====
* PSP ist in Kubernetes 1.21 veraltet und wird in 1.25 vollständig entfernt. 
* Die Manager- und Scanner-Pods laufen ohne Flüssigkeit. Wenn Ihr PSP eine Regel hat, `Als Benutzer ausführen: Regel: MustRunAsNonRoot` fügt dann folgendes in das Beispiel yaml unten (mit entsprechendem Wert für `###`):
====
+
--
[,yaml]
----
securityContext:
    runAsUser: ###
----

Beschriften Sie für PSA in Kubernetes 1.25+ den {product-name} mit einem privilegierten Profil für die Bereitstellung auf einem PSA-fähigen Cluster. 
[,shell]
----
kubectl label namespace neuvector "pod-security.kubernetes.io/enforce=privileged" 
----
--
. Erstellen Sie benutzerdefinierte Ressourcen (CRD) für {product-name}. Für Kubernetes 1.19+: 
+
--
[,shell]
----
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/waf-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/dlp-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/com-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/vul-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/admission-crd-k8s-1.19.yaml
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/5.4.3_group-definition-k8s.yaml
----
--
. Fügen Sie Leseberechtigung hinzu, um auf die kubernetes API zuzugreifen. 
+
[IMPORTANT]
====
Die Standardbereitstellung von {product-name} 5.2+ verwendet anstelle der Standardeinstellung die am wenigsten privilegierten Dienstkonten. Siehe unten, ob ein Upgrade von einer Version vor 5.3. 
====
+
[WARNING]
====
Wenn Sie ein Upgrade auf 5.3.0+ durchführen, führen Sie die folgenden Befehle basierend auf Ihrer aktuellen Version aus:
====
+
--
[tabs]
======
Version 5.2.0::
+
====
[,shell]
----
kubectl delete clusterrole neuvector-binding-nvsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-nvwafsecurityrules 
----
====

Versionen vor Version 5.2.0::
+
====
[,shell]
----
kubectl delete clusterrolebinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules
kubectl delete rolebinding neuvector-admin -n neuvector 
----
====
======

Wenden Sie die Leserechte über die folgenden Befehle an: 

[,shell]
----
kubectl create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces
kubectl create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io
kubectl create clusterrolebinding neuvector-binding-app --clusterrole=neuvector-binding-app --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-rbac --clusterrole=neuvector-binding-rbac --serviceaccount=neuvector:controller
kubectl create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations
kubectl create clusterrolebinding neuvector-binding-admission --clusterrole=neuvector-binding-admission --serviceaccount=neuvector:controller
kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions
kubectl create clusterrolebinding neuvector-binding-customresourcedefinition --clusterrole=neuvector-binding-customresourcedefinition --serviceaccount=neuvector:controller
kubectl create clusterrole neuvector-binding-nvsecurityrules --verb=get,list,delete --resource=nvsecurityrules,nvclustersecurityrules
kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=get,list,delete --resource=nvadmissioncontrolsecurityrules
kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=get,list,delete --resource=nvdlpsecurityrules
kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=get,list,delete --resource=nvwafsecurityrules
kubectl create clusterrolebinding neuvector-binding-nvsecurityrules --clusterrole=neuvector-binding-nvsecurityrules --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:controller
kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:controller
kubectl create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector
kubectl create rolebinding neuvector-binding-scanner --role=neuvector-binding-scanner --serviceaccount=neuvector:updater --serviceaccount=neuvector:controller -n neuvector
kubectl create role neuvector-binding-secret --verb=get --resource=secrets -n neuvector
kubectl create rolebinding neuvector-binding-secret --role=neuvector-binding-secret --serviceaccount=neuvector:controller -n neuvector
kubectl create role neuvector-binding-secret --verb=get,list,watch --resource=secrets -n neuvector
kubectl create rolebinding neuvector-binding-secret --role=neuvector-binding-secret --serviceaccount=neuvector:controller --serviceaccount=neuvector:enforcer --serviceaccount=neuvector:scanner --serviceaccount=neuvector:registry-adapter -n neuvector
kubectl create clusterrole neuvector-binding-nvcomplianceprofiles --verb=get,list,delete --resource=nvcomplianceprofiles
kubectl create clusterrolebinding neuvector-binding-nvcomplianceprofiles --clusterrole=neuvector-binding-nvcomplianceprofiles --serviceaccount=neuvector:controller
kubectl create clusterrole neuvector-binding-nvvulnerabilityprofiles --verb=get,list,delete --resource=nvvulnerabilityprofiles
kubectl create clusterrolebinding neuvector-binding-nvvulnerabilityprofiles --clusterrole=neuvector-binding-nvvulnerabilityprofiles --serviceaccount=neuvector:controller 
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/neuvector-roles-k8s.yaml
kubectl create role neuvector-binding-lease --verb=create,get,update --resource=leases -n neuvector
kubectl create rolebinding neuvector-binding-cert-upgrader --role=neuvector-binding-cert-upgrader --serviceaccount=neuvector:cert-upgrader -n neuvector
kubectl create rolebinding neuvector-binding-job-creation --role=neuvector-binding-job-creation --serviceaccount=neuvector:controller -n neuvector
kubectl create rolebinding neuvector-binding-lease --role=neuvector-binding-lease --serviceaccount=neuvector:controller --serviceaccount=neuvector:cert-upgrader -n neuvector
kubectl create clusterrole neuvector-binding-nvgroupdefinitions --verb=list,get,delete --resource=nvgroupdefinitions
kubectl create clusterrolebinding neuvector-binding-nvgroupdefinitions --clusterrole=neuvector-binding-nvgroupdefinitions --serviceaccount=neuvector:controller
----
--
. Führen Sie die folgenden Befehle aus, um zu überprüfen, ob die Dienstkonten neuvector/controller und neuvector/updater erfolgreich hinzugefügt wurden. 
+
--
[,shell]
----
kubectl get ClusterRoleBinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-nvgroupdefinitions -o wide
----

Beispielausgabe: 

[,shell]
----
NAME                                                ROLE                                                            AGE   USERS   GROUPS   SERVICEACCOUNTS
neuvector-binding-app                               ClusterRole/neuvector-binding-app                               45s                    neuvector/controller
neuvector-binding-rbac                              ClusterRole/neuvector-binding-rbac                              45s                    neuvector/controller
neuvector-binding-admission                         ClusterRole/neuvector-binding-admission                         44s                    neuvector/controller
neuvector-binding-customresourcedefinition          ClusterRole/neuvector-binding-customresourcedefinition          44s                    neuvector/controller
neuvector-binding-nvsecurityrules                   ClusterRole/neuvector-binding-nvsecurityrules                   43s                    neuvector/controller
neuvector-binding-view                              ClusterRole/view                                                43s                    neuvector/controller
neuvector-binding-nvwafsecurityrules                ClusterRole/neuvector-binding-nvwafsecurityrules                43s                    neuvector/controller
neuvector-binding-nvadmissioncontrolsecurityrules   ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules   43s                    neuvector/controller
neuvector-binding-nvdlpsecurityrules                ClusterRole/neuvector-binding-nvdlpsecurityrules                43s                    neuvector/controller
neuvector-binding-nvgroupdefinitions                ClusterRole/neuvector-binding-nvgroupdefinitions                40s                    neuvector/controller
----

Und dieser Befehl: 

[,shell]
----
kubectl get RoleBinding neuvector-binding-scanner neuvector-binding-cert-upgrader neuvector-binding-job-creation neuvector-binding-lease neuvector-binding-secret -n neuvector -o wide
----

Beispielausgabe: 

[,shell]
----
NAME                              ROLE                                   AGE    USERS   GROUPS   SERVICEACCOUNTS
neuvector-binding-scanner         Role/neuvector-binding-scanner         8m8s                    neuvector/controller, neuvector/updater
neuvector-binding-cert-upgrader   Role/neuvector-binding-cert-upgrader   8m8s                    neuvector/cert-upgrader
neuvector-binding-job-creation    Role/neuvector-binding-job-creation    8m8s                    neuvector/controller
neuvector-binding-lease           Role/neuvector-binding-lease           8m8s                    neuvector/controller, neuvector/cert-upgrader
neuvector-binding-secret          Role/neuvector-binding-secret          8m8s                    neuvector/controller, neuvector/enforcer, neuvector/scanner, neuvector/registry-adapter
----
--
. (*Optional*) Erstellen Sie die Federation Master- und/oder Remote-Multiclusterverwaltungsdienste. Wenn Sie die Multiclusterverwaltungsfunktionen in {product-name} verwenden möchten, muss in einem Cluster der Federation Master-Dienst bereitgestellt werden, und in jedem Remotecluster muss der Federation Worker-Dienst vorhanden sein. Aus Flexibilitätsgründen können Sie für jeden Cluster sowohl Master- als auch Worker-Dienste bereitstellen, sodass jeder Cluster ein Master oder Remote sein kann. Föderiertes Clustermanagement 
+
--
[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-master
  namespace: neuvector
spec:
  ports:
  - port: 11443
    name: fed
    protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-worker
  namespace: neuvector
spec:
  ports:
  - port: 10443
    name: fed
    protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-controller-pod
----

Dann erstellen Sie die entsprechenden Dienste: 

[,shell]
----
kubectl create -f nv_master_worker.yaml 
----
--
. Erstellen Sie die primären {product-name} und Pods mit den voreingestellten Versionsbefehlen oder ändern Sie den Beispiel-Yaml unten. Die voreingestellte Version ruft einen LoadBalancer für die {product-name} Console auf. Wenn Sie die Beispiel-Yaml-Datei unten verwenden, ersetzen Sie die Bildnamen und <version> für die Manager-, Controller- und Enforcer-Bildreferenzen in der Yaml-Datei. Nehmen Sie auch alle anderen Änderungen vor, die für Ihre Bereitstellungsumgebung erforderlich sind (z. B. LoadBalancer/NodePort/Ingress für Managerzugriff usw.). Die folgende YAML muss für interne Zertifikatsänderungen geändert werden, wenn sie ab Version 5.4.2 bereitgestellt wird. Siehe dieses <<_kubernetes_deployment_yaml_for_v5_4_2_onwards,YAML>>.
+
--
[,shell]
----
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/neuvector-k8s.yaml 
----

Oder, wenn Sie einen der obigen Yaml oder Proben von unten ändern: 

[,shell]
----
kubectl create -f neuvector.yaml 
----

Fertig. Sie sollten sich mit der {product-name} verbinden und mit admin:admin anmelden können, z.B. `+https://<public-ip>:8443+`
--

[NOTE]
====
Der in der Datei neuvector.yaml angegebene nodeport-Dienst öffnet einen zufälligen Port auf allen kubernetes-Knoten für den {product-name}-Management-Webkonsolenport. Alternativ können Sie einen LoadBalancer oder Ingress verwenden, der eine öffentliche IP und einen Standardport 8443 verwendet. Stellen Sie bei nodeport sicher, dass Sie den Zugriff über Firewalls für diesen Port öffnen, falls erforderlich. Wenn Sie sehen möchten, welcher Port auf den Hostknoten geöffnet ist, führen Sie bitte die folgenden Befehle aus:

[,shell]
----
kubectl get svc -n neuvector
----

Und Sie werden so etwas sehen wie:

[,shell]
----
NAME                          CLUSTER-IP      EXTERNAL-IP   PORT(S)                                          AGE
neuvector-service-webui     10.100.195.99     <nodes>       8443:30257/TCP                                   15m
----
====

*PKS Ändern*

[NOTE]
====
PKS ist feldgetestet und erfordert das Aktivieren von privilegierten Containern für den Plan/die Kachel und das Ändern des yaml hostPath wie folgt für Allinone, Controller, Enforcer:

[,yaml]
----
      hostPath:
            path: /var/vcap/sys/run/docker/docker.sock
----
====

*Master Node Taints und Toleranzen*

Alle Fleckinformationen müssen übereinstimmen, um Vollstrecker auf Knoten zu planen. So überprüfen Sie die Fleckinformationen auf einem Knoten (z.B. Master):

[,shell]
----
kubectl get node taintnodename -o yaml
----

Beispielausgabe:

[,yaml]
----
spec:
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  # there may be an extra info for taint as below
  - effect: NoSchedule
    key: mykey
    value: myvalue
----

Wenn es zusätzliche Flecken wie oben gibt, fügen Sie diese dem Abschnitt Proben-Yaml-Verträglichkeiten hinzu:

[,yaml]
----
spec:
  template:
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        # if there is an extra info for taints as above, please add it here. This is required to match all the taint info defined on the taint node. Otherwise, the Enforcer won't deploy on the taint node
        - effect: NoSchedule
          key: mykey
          value: myvalue
----

== Verwenden von Knotenbeschriftungen für Manager- und Controllerknoten

Um zu steuern, auf welchen Knoten der Manager und Controller bereitgestellt werden, markieren Sie jeden Knoten. Ersetzen Sie den Knotennamen durch den entsprechenden Knotennamen ('`kubectl get nodes`'). Hinweis: Standardmäßig plant Kubernetes keine Pods auf dem Masterknoten.

[,shell]
----
kubectl label nodes nodename nvcontroller=true
----

Fügen Sie dann der yaml-Datei für die Bereitstellungsabschnitte Manager und Controller einen nodeSelector hinzu. Beispiel:

[,yaml]
----
          - mountPath: /host/cgroup
              name: cgroup-vol
              readOnly: true
      nodeSelector:
        nvcontroller: "true"
      restartPolicy: Always
----

Um zu verhindern, dass der Enforcer auf einem Controller-Knoten bereitgestellt wird, fügen Sie dem Enforcer-yaml-Abschnitt einen nodeAffinity hinzu, wenn es sich um einen dedizierten Verwaltungsknoten (ohne zu überwachende Anwendungscontainer) handelt. Beispiel:

[,yaml]
----
  app: neuvector-enforcer-pod
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: nvcontroller
                  operator: NotIn
                  values: ["true"]
      imagePullSecrets:
----

== Rollende Updates

Orchestrierungstools wie Kubernetes, RedHat OpenShift und Rancher unterstützen rollierende Updates mit konfigurierbaren Richtlinien. Mit dieser Funktion können Sie die {product-name} aktualisieren. Das wichtigste wird sein, sicherzustellen, dass mindestens ein Controller (oder Allinone) ausgeführt wird, damit Richtlinien, Protokolle und Verbindungsdaten nicht verloren gehen. Stellen Sie sicher, dass zwischen Container-Updates mindestens 120 Sekunden liegen, damit ein neuer Leiter gewählt und die Daten zwischen Controllern synchronisiert werden können.

Die bereitgestellten Beispielbereitstellungs-Yamls konfigurieren bereits die rollierende Updaterichtlinie. Wenn Sie über das {product-name}-Helm-Diagramm aktualisieren, ziehen Sie bitte das neueste Diagramm, um neue Funktionen wie die Zulassungskontrolle ordnungsgemäß zu konfigurieren, und löschen Sie die alte Clusterrolle und Clusterrollenbindung für {product-name}. Wenn Sie über Kubernetes aktualisieren, können Sie manuell mit den folgenden Beispielbefehlen auf eine neue Version aktualisieren.

=== Beispiel Kubernetes Rolling Update

Für Upgrades, die nur auf eine neue Imageversion aktualisiert werden müssen, können Sie diesen einfachen Ansatz verwenden.

Wenn Ihr Deployment oder Daemonset bereits ausgeführt wird, können Sie die yaml-Datei auf die neue Version ändern und dann das Update anwenden:

[,shell]
----
kubectl apply -f <yaml file>
----

So aktualisieren Sie {product-name} über die Befehlszeile auf eine neue Version.

Für Controller als Deployment (auch für Manager tun)

[,shell]
----
kubectl set image deployment/neuvector-controller-pod neuvector-controller-pod=neuvector/controller:<version> -n neuvector
----

Für jeden Container als DaemonSet:

[,shell]
----
kubectl set image -n neuvector ds/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:<version>
----

So überprüfen Sie den Status des rollierenden Updates:

[,shell]
----
kubectl rollout status -n neuvector ds/neuvector-enforcer-pod
kubectl rollout status -n neuvector deployment/neuvector-controller-pod
----

So rollt man das Update zurück:

[,shell]
----
kubectl rollout undo -n neuvector ds/neuvector-enforcer-pod
kubectl rollout undo -n neuvector deployment/neuvector-controller-pod
----

== REST API in Kubernetes ausstellen

Um die REST-API für den Zugriff von außerhalb des Kubernetes-Clusters freizulegen, hier eine Beispiel-Yaml-Datei:

[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-rest
  namespace: neuvector
spec:
  ports:
    - port: 10443
      name: controller
      protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-controller-pod
----

Weitere Informationen zur REST API finden Sie im Abschnitt Automatisierung.

== Kubernetes-Bereitstellung im nichtprivilegierten Modus

Die folgenden Anweisungen können verwendet werden, um {product-name} bereitzustellen, ohne privilegierte Moduscontainer zu verwenden. Der Controller befindet sich bereits im nicht privilegierten Modus und die Enforcer-Bereitstellung sollte geändert werden, was in den untenstehenden auszugsweise dargestellten Snippets angezeigt wird.

Enforcer:

[,yaml]
----
spec:
  template:
    metadata:
      annotations:
        container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined
        # this line is required to be added if k8s version is pre-v1.19
        # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined
    spec:
      containers:
          securityContext:
            # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show
            seccompProfile:
              type: Unconfined
            capabilities:
              add:
              - SYS_ADMIN
              - NET_ADMIN
              - SYS_PTRACE
              - IPC_LOCK
----

== Kubernetes Deployment YAML ab Version 5.4.2

Das folgende Beispiel YAML ist für Versionen 5.4.2 und höher, wo wir die internen Zertifikate auf Controller, Enforcer und Scanner-Pods mounten müssen, da wir keine festkodierten Zertifikate mehr unterstützen. Erstellen Sie das interne Zertifikatsgeheimnis aus dem angegebenen Link, bevor Sie Folgendes bereitstellen: xref:internal.adoc[Ersatz interner Zertifikate].

.Klicken Sie hier für Details
[%collapsible]
====
[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-crd-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 30443
    protocol: TCP
    name: crd-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-admission-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 20443
    protocol: TCP
    name: admission-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-webui
  namespace: neuvector
spec:
  ports:
    - port: 8443
      name: manager
      protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-manager-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-controller
  namespace: neuvector
spec:
  ports:
  - port: 18300
    protocol: "TCP"
    name: "cluster-tcp-18300"
  - port: 18301
    protocol: "TCP"
    name: "cluster-tcp-18301"
  - port: 18301
    protocol: "UDP"
    name: "cluster-udp-18301"
  clusterIP: None
  selector:
    app: neuvector-controller-pod

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-manager-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-manager-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: neuvector-manager-pod
    spec:
      serviceAccountName: basic
      serviceAccount: basic
      containers:
        - name: neuvector-manager-pod
          image: neuvector/manager:5.4.3
          env:
            - name: CTRL_SERVER_IP
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-controller-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-controller-pod
  minReadySeconds: 60
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 3
  template:
    metadata:
      labels:
        app: neuvector-controller-pod
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - neuvector-controller-pod
              topologyKey: "kubernetes.io/hostname"
      serviceAccountName: controller
      serviceAccount: controller
      containers:
        - name: neuvector-controller-pod
          image: neuvector/controller:5.4.3
          securityContext:
            runAsUser: 0
          readinessProbe:
            exec:
              command:
              - cat
              - /tmp/ready
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /etc/config
              name: config-volume
              readOnly: true
            - mountPath: /etc/neuvector/certs/internal/cert.key
              name: internal-cert
              readOnly: true
              subPath: tls.key
            - mountPath: /etc/neuvector/certs/internal/cert.pem
              name: internal-cert
              readOnly: true
              subPath: tls.crt
            - mountPath: /etc/neuvector/certs/internal/ca.cert
              name: internal-cert
              readOnly: true
              subPath: ca.crt
      terminationGracePeriodSeconds: 300
      restartPolicy: Always
      volumes:
        - name: config-volume
          projected:
            sources:
              - configMap:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-secret
                  optional: true
        - name: internal-cert
          secret:
            defaultMode: 420
            secretName: internal-cert

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: neuvector-enforcer-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-enforcer-pod
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: neuvector-enforcer-pod
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
      hostPID: true
      serviceAccountName: enforcer
      serviceAccount: enforcer
      containers:
        - name: neuvector-enforcer-pod
          image: neuvector/enforcer:5.4.3
          securityContext:
            privileged: true
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /lib/modules
              name: modules-vol
              readOnly: true
            - mountPath: /var/nv_debug
              name: nv-debug
              readOnly: false
            - mountPath: /etc/neuvector/certs/internal/cert.key
              name: internal-cert
              readOnly: true
              subPath: tls.key
            - mountPath: /etc/neuvector/certs/internal/cert.pem
              name: internal-cert
              readOnly: true
              subPath: tls.crt
            - mountPath: /etc/neuvector/certs/internal/ca.cert
              name: internal-cert
              readOnly: true
              subPath: ca.crt
      terminationGracePeriodSeconds: 1200
      restartPolicy: Always
      volumes:
        - name: modules-vol
          hostPath:
            path: /lib/modules
        - name: nv-debug
          hostPath:
            path: /var/nv_debug
        - name: internal-cert
          secret:
            defaultMode: 420
            secretName: internal-cert

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-scanner-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-scanner-pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 2
  template:
    metadata:
      labels:
        app: neuvector-scanner-pod
    spec:
      serviceAccountName: scanner
      serviceAccount: scanner
      containers:
        - name: neuvector-scanner-pod
          image: neuvector/scanner:latest
          imagePullPolicy: Always
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
          volumeMounts:
            - mountPath: /etc/neuvector/certs/internal/cert.key
              name: internal-cert
              readOnly: true
              subPath: tls.key
            - mountPath: /etc/neuvector/certs/internal/cert.pem
              name: internal-cert
              readOnly: true
              subPath: tls.crt
            - mountPath: /etc/neuvector/certs/internal/ca.cert
              name: internal-cert
              readOnly: true
              subPath: ca.crt
      restartPolicy: Always
      volumes:
        - name: internal-cert
          secret:
            defaultMode: 420
            secretName: internal-cert
---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: neuvector-updater-pod
  namespace: neuvector
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: neuvector-updater-pod
        spec:
          serviceAccountName: updater
          serviceAccount: updater
          containers:
          - name: neuvector-updater-pod
            image: neuvector/updater:latest
            imagePullPolicy: Always
            command:
            - /bin/sh
            - -c
            - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H "Authorization:Bearer $TOKEN" -H "Content-Type:application/strategic-merge-patch+json" -d '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt":"'`date +%Y-%m-%dT%H:%M:%S%z`'"}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod'
          restartPolicy: Never
----
====

Das folgende Beispiel ist eine vollständige Bereitstellungsreferenz (Kubernetes 1.19+).

.Klicken Sie hier für Details
[%collapsible]
====
[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-crd-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 30443
    protocol: TCP
    name: crd-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-admission-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 20443
    protocol: TCP
    name: admission-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-webui
  namespace: neuvector
spec:
  ports:
    - port: 8443
      name: manager
      protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-manager-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-controller
  namespace: neuvector
spec:
  ports:
  - port: 18300
    protocol: "TCP"
    name: "cluster-tcp-18300"
  - port: 18301
    protocol: "TCP"
    name: "cluster-tcp-18301"
  - port: 18301
    protocol: "UDP"
    name: "cluster-udp-18301"
  clusterIP: None
  selector:
    app: neuvector-controller-pod

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-manager-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-manager-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: neuvector-manager-pod
    spec:
      serviceAccountName: basic
      serviceAccount: basic
      containers:
        - name: neuvector-manager-pod
          image: neuvector/manager:5.4.3
          env:
            - name: CTRL_SERVER_IP
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-controller-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-controller-pod
  minReadySeconds: 60
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 3
  template:
    metadata:
      labels:
        app: neuvector-controller-pod
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - neuvector-controller-pod
              topologyKey: "kubernetes.io/hostname"
      serviceAccountName: controller
      serviceAccount: controller
      containers:
        - name: neuvector-controller-pod
          image: neuvector/controller:5.4.3
          securityContext:
            runAsUser: 0
          readinessProbe:
            exec:
              command:
              - cat
              - /tmp/ready
            initialDelaySeconds: 5
            periodSeconds: 5
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /etc/config
              name: config-volume
              readOnly: true
      terminationGracePeriodSeconds: 300
      restartPolicy: Always
      volumes:
        - name: config-volume
          projected:
            sources:
              - configMap:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-secret
                  optional: true

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: neuvector-enforcer-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-enforcer-pod
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: neuvector-enforcer-pod
      annotations:
        container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined
      # Add the following for pre-v1.19
      # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
      hostPID: true
      serviceAccountName: enforcer
      serviceAccount: enforcer
      containers:
        - name: neuvector-enforcer-pod
          image: neuvector/enforcer:5.4.3
          securityContext:
            # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show
            seccompProfile:
              type: Unconfined
            capabilities:
              add:
              - SYS_ADMIN
              - NET_ADMIN
              - SYS_PTRACE
              - IPC_LOCK
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /lib/modules
              name: modules-vol
              readOnly: true
            - mountPath: /var/nv_debug
              name: nv-debug
              readOnly: false
      terminationGracePeriodSeconds: 1200
      restartPolicy: Always
      volumes:
        - name: modules-vol
          hostPath:
            path: /lib/modules
        - name: nv-debug
          hostPath:
            path: /var/nv_debug

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-scanner-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-scanner-pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 2
  template:
    metadata:
      labels:
        app: neuvector-scanner-pod
    spec:
      serviceAccountName: scanner
      serviceAccount: scanner
      containers:
        - name: neuvector-scanner-pod
          image: neuvector/scanner:latest
          imagePullPolicy: Always
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: neuvector-updater-pod
  namespace: neuvector
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: neuvector-updater-pod
        spec:
          serviceAccountName: updater
          serviceAccount: updater
          containers:
          - name: neuvector-updater-pod
            image: neuvector/updater:latest
            imagePullPolicy: Always
            command:
            - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H "Authorization:Bearer $TOKEN" -H "Content-Type:application/strategic-merge-patch+json" -d '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt":"'`date +%Y-%m-%dT%H:%M:%S%z`'"}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod'
          restartPolicy: Never
----
====

== PKS Change

[NOTE]
====
PKS ist feldgetestet und erfordert das Aktivieren von privilegierten Containern für den Plan/die Kachel und das Ändern des yaml hostPath wie folgt für Allinone, Enforcer:

[,yaml]
----
      hostPath:
            path: /var/vcap/sys/run/docker/docker.sock
----
====
